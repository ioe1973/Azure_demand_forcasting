{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure Demand Forecasting - Data Loading & EDA\n",
    "\n",
    "This notebook covers:\n",
    "1. Loading the raw datasets\n",
    "2. Basic data exploration\n",
    "3. Data cleaning steps\n",
    "4. Initial exploratory data analysis\n",
    "\n",
    "**Author**: SCSBalaji  \n",
    "**Date**: 2025-08-27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Date handling\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options for better readability\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Raw Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Azure usage data\n",
    "print(\"ðŸ“‚ Loading Azure usage data...\")\n",
    "azure_usage = pd.read_csv('../data/raw/azure_usage.csv')\n",
    "print(f\"Azure usage data shape: {azure_usage.shape}\")\n",
    "\n",
    "# Load external factors data\n",
    "print(\"\\nðŸ“‚ Loading external factors data...\")\n",
    "external_factors = pd.read_csv('../data/raw/external_factors.csv')\n",
    "print(f\"External factors data shape: {external_factors.shape}\")\n",
    "\n",
    "print(\"\\nâœ… Both datasets loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initial Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore Azure usage data\n",
    "print(\"=== AZURE USAGE DATA OVERVIEW ===\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(azure_usage.head())\n",
    "\n",
    "print(\"\\nDataset Info:\")\n",
    "print(azure_usage.info())\n",
    "\n",
    "print(\"\\nColumn Names:\")\n",
    "print(azure_usage.columns.tolist())\n",
    "\n",
    "print(\"\\nBasic Statistics:\")\n",
    "print(azure_usage.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore External factors data\n",
    "print(\"=== EXTERNAL FACTORS DATA OVERVIEW ===\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(external_factors.head())\n",
    "\n",
    "print(\"\\nDataset Info:\")\n",
    "print(external_factors.info())\n",
    "\n",
    "print(\"\\nColumn Names:\")\n",
    "print(external_factors.columns.tolist())\n",
    "\n",
    "print(\"\\nBasic Statistics:\")\n",
    "print(external_factors.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Check for Data Quality Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in Azure usage data\n",
    "print(\"=== MISSING VALUES CHECK ===\")\n",
    "print(\"\\nAzure Usage Data:\")\n",
    "missing_azure = azure_usage.isnull().sum()\n",
    "print(missing_azure)\n",
    "\n",
    "print(\"\\nExternal Factors Data:\")\n",
    "missing_external = external_factors.isnull().sum()\n",
    "print(missing_external)\n",
    "\n",
    "# Calculate percentage of missing values\n",
    "print(\"\\n=== MISSING VALUES PERCENTAGE ===\")\n",
    "print(\"Azure Usage:\")\n",
    "print((missing_azure / len(azure_usage)) * 100)\n",
    "print(\"\\nExternal Factors:\")\n",
    "print((missing_external / len(external_factors)) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    "print(\"=== DUPLICATE ROWS CHECK ===\")\n",
    "azure_duplicates = azure_usage.duplicated().sum()\n",
    "external_duplicates = external_factors.duplicated().sum()\n",
    "\n",
    "print(f\"Azure usage duplicates: {azure_duplicates}\")\n",
    "print(f\"External factors duplicates: {external_duplicates}\")\n",
    "\n",
    "# Check for unique values in categorical columns\n",
    "print(\"\\n=== UNIQUE VALUES IN KEY COLUMNS ===\")\n",
    "print(f\"Unique regions: {azure_usage['region'].unique()}\")\n",
    "print(f\"Unique resource types: {azure_usage['resource_type'].unique()}\")\n",
    "print(f\"Date range (Azure): {azure_usage['date'].min()} to {azure_usage['date'].max()}\")\n",
    "print(f\"Date range (External): {external_factors['date'].min()} to {external_factors['date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Cleaning Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Convert date columns to datetime\n",
    "print(\"ðŸ”§ Step 1: Standardizing date formats...\")\n",
    "\n",
    "# Convert date columns\n",
    "azure_usage['date'] = pd.to_datetime(azure_usage['date'])\n",
    "external_factors['date'] = pd.to_datetime(external_factors['date'])\n",
    "\n",
    "print(\"âœ… Date columns converted to datetime format\")\n",
    "print(f\"Azure date type: {azure_usage['date'].dtype}\")\n",
    "print(f\"External date type: {external_factors['date'].dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Handle missing values (if any)\n",
    "print(\"ðŸ”§ Step 2: Handling missing values...\")\n",
    "\n",
    "# For this dataset, let's check if there are any missing values and handle them\n",
    "azure_before = len(azure_usage)\n",
    "external_before = len(external_factors)\n",
    "\n",
    "# Remove rows with missing values (you can also choose to fill them)\n",
    "azure_usage_clean = azure_usage.dropna()\n",
    "external_factors_clean = external_factors.dropna()\n",
    "\n",
    "azure_after = len(azure_usage_clean)\n",
    "external_after = len(external_factors_clean)\n",
    "\n",
    "print(f\"Azure data: {azure_before} -> {azure_after} rows (removed {azure_before - azure_after})\")\n",
    "print(f\"External data: {external_before} -> {external_after} rows (removed {external_before - external_after})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Remove duplicate rows\n",
    "print(\"ðŸ”§ Step 3: Removing duplicate rows...\")\n",
    "\n",
    "azure_before_dup = len(azure_usage_clean)\n",
    "external_before_dup = len(external_factors_clean)\n",
    "\n",
    "azure_usage_clean = azure_usage_clean.drop_duplicates()\n",
    "external_factors_clean = external_factors_clean.drop_duplicates()\n",
    "\n",
    "azure_after_dup = len(azure_usage_clean)\n",
    "external_after_dup = len(external_factors_clean)\n",
    "\n",
    "print(f\"Azure data: {azure_before_dup} -> {azure_after_dup} rows (removed {azure_before_dup - azure_after_dup} duplicates)\")\n",
    "print(f\"External data: {external_before_dup} -> {external_after_dup} rows (removed {external_before_dup - external_after_dup} duplicates)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Data validation and normalization\n",
    "print(\"ðŸ”§ Step 4: Data validation and normalization...\")\n",
    "\n",
    "# Check for any negative values or outliers\n",
    "print(\"\\nChecking for negative values:\")\n",
    "numeric_cols = ['usage_cpu', 'usage_storage', 'users_active']\n",
    "for col in numeric_cols:\n",
    "    negative_count = (azure_usage_clean[col] < 0).sum()\n",
    "    print(f\"{col}: {negative_count} negative values\")\n",
    "\n",
    "# Check data ranges\n",
    "print(\"\\nData ranges:\")\n",
    "print(f\"CPU usage range: {azure_usage_clean['usage_cpu'].min()} - {azure_usage_clean['usage_cpu'].max()}\")\n",
    "print(f\"Storage usage range: {azure_usage_clean['usage_storage'].min()} - {azure_usage_clean['usage_storage'].max()}\")\n",
    "print(f\"Active users range: {azure_usage_clean['users_active'].min()} - {azure_usage_clean['users_active'].max()}\")\n",
    "\n",
    "print(\"\\nâœ… Data cleaning completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Basic Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average daily CPU usage per region\n",
    "print(\"ðŸ“Š ANALYSIS 1: Average Daily CPU Usage per Region\")\n",
    "avg_cpu_by_region = azure_usage_clean.groupby('region')['usage_cpu'].mean().sort_values(ascending=False)\n",
    "print(avg_cpu_by_region)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "avg_cpu_by_region.plot(kind='bar', color='skyblue')\n",
    "plt.title('Average CPU Usage by Region')\n",
    "plt.xlabel('Region')\n",
    "plt.ylabel('Average CPU Usage (%)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate peak demand per month\n",
    "print(\"ðŸ“Š ANALYSIS 2: Peak Demand per Month\")\n",
    "\n",
    "# Add month column\n",
    "azure_usage_clean['month'] = azure_usage_clean['date'].dt.to_period('M')\n",
    "\n",
    "# Calculate peak CPU usage per month\n",
    "peak_cpu_by_month = azure_usage_clean.groupby('month')['usage_cpu'].max()\n",
    "print(\"Peak CPU usage by month:\")\n",
    "print(peak_cpu_by_month)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 6))\n",
    "peak_cpu_by_month.plot(kind='line', marker='o', color='red', linewidth=2)\n",
    "plt.title('Peak CPU Usage by Month')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Peak CPU Usage (%)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find top 5 regions by total usage\n",
    "print(\"ðŸ“Š ANALYSIS 3: Top 5 Regions by Total Usage\")\n",
    "\n",
    "# Calculate total usage (sum of CPU + Storage + Users)\n",
    "azure_usage_clean['total_usage'] = (azure_usage_clean['usage_cpu'] + \n",
    "                                   azure_usage_clean['usage_storage'] + \n",
    "                                   azure_usage_clean['users_active'])\n",
    "\n",
    "top_regions = azure_usage_clean.groupby('region')['total_usage'].sum().sort_values(ascending=False).head(5)\n",
    "print(\"Top 5 regions by total usage:\")\n",
    "print(top_regions)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_regions.plot(kind='bar', color='green')\n",
    "plt.title('Top 5 Regions by Total Usage')\n",
    "plt.xlabel('Region')\n",
    "plt.ylabel('Total Usage')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Additional Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage trends over time\n",
    "print(\"ðŸ“Š ADDITIONAL ANALYSIS: Usage Trends Over Time\")\n",
    "\n",
    "# Group by date and calculate daily averages\n",
    "daily_usage = azure_usage_clean.groupby('date').agg({\n",
    "    'usage_cpu': 'mean',\n",
    "    'usage_storage': 'mean',\n",
    "    'users_active': 'mean'\n",
    "})\n",
    "\n",
    "# Plot trends\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "daily_usage['usage_cpu'].plot(color='blue')\n",
    "plt.title('Daily Average CPU Usage')\n",
    "plt.ylabel('CPU Usage (%)')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "daily_usage['usage_storage'].plot(color='orange')\n",
    "plt.title('Daily Average Storage Usage')\n",
    "plt.ylabel('Storage Usage (GB)')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "daily_usage['users_active'].plot(color='green')\n",
    "plt.title('Daily Average Active Users')\n",
    "plt.ylabel('Active Users')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "# Resource type distribution\n",
    "azure_usage_clean['resource_type'].value_counts().plot(kind='pie', autopct='%1.1f%%')\n",
    "plt.title('Resource Type Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary of Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== DATA CLEANING SUMMARY ===\")\n",
    "print(f\"âœ… Azure usage dataset: {len(azure_usage_clean)} records\")\n",
    "print(f\"âœ… External factors dataset: {len(external_factors_clean)} records\")\n",
    "print(f\"âœ… Date range: {azure_usage_clean['date'].min().date()} to {azure_usage_clean['date'].max().date()}\")\n",
    "print(f\"âœ… Regions covered: {azure_usage_clean['region'].nunique()}\")\n",
    "print(f\"âœ… Resource types: {azure_usage_clean['resource_type'].nunique()}\")\n",
    "\n",
    "print(\"\\n=== KEY INSIGHTS ===\")\n",
    "print(f\"ðŸ”¹ Highest average CPU usage region: {avg_cpu_by_region.index[0]} ({avg_cpu_by_region.iloc[0]:.1f}%)\")\n",
    "print(f\"ðŸ”¹ Peak CPU usage recorded: {azure_usage_clean['usage_cpu'].max()}%\")\n",
    "print(f\"ðŸ”¹ Most active region by total usage: {top_regions.index[0]}\")\n",
    "\n",
    "print(\"\\nâœ… Data is now clean and ready for merging with external factors!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Cleaned Datasets (Optional)\n",
    "Uncomment the lines below if you want to save the cleaned individual datasets before merging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned datasets for future use\n",
    "# azure_usage_clean.to_csv('../data/processed/azure_usage_cleaned.csv', index=False)\n",
    "# external_factors_clean.to_csv('../data/processed/external_factors_cleaned.csv', index=False)\n",
    "# print(\"âœ… Cleaned datasets saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}